{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e466b6fa16c947ffa40112096aafb7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset common_voice_13_0 (/home/dev/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/bn/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055)\n",
      "Found cached dataset common_voice_13_0 (/home/dev/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/bn/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric, Audio\n",
    "\n",
    "common_voice_train = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"bn\", split=\"train\", use_auth_token=True)\n",
    "common_voice_test = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"bn\", split=\"test\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\", \"variant\"])\n",
    "common_voice_test = common_voice_test.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\", \"variant\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_remove_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\'\\‚\\/]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"sentence\"] = re.sub(chars_to_remove_regex, '', batch[\"sentence\"]).lower()\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ccc2096c96c44c0aa5e18d2c4f7ff6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20729 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546cbd325bbe44a0b08f46dfa539fa82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9230 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "common_voice_train = common_voice_train.map(remove_special_characters)\n",
    "common_voice_test = common_voice_test.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import DatasetDict\n",
    "labels_df = pd.read_csv(\"bengaliai-speech/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df = labels_df[labels_df[\"split\"]==\"train\"]\n",
    "val_df = labels_df[labels_df[\"split\"]==\"valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import Audio\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, labels_df: pd.DataFrame, data_path: str, sampling_rate: int =32_000):\n",
    "        self.labels_df = labels_df\n",
    "        self.data_path = data_path\n",
    "        self.sampling_rate = sampling_rate\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "    def __getitem__(self,idx):\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        path = os.path.join(self.data_path, row[\"id\"]+\".mp3\")\n",
    "        sentence = row[\"sentence\"]\n",
    "        with open(path, \"rb\") as f:\n",
    "            speech = f.read()\n",
    "            audio = Audio(sampling_rate=self.sampling_rate).decode_example({\"path\": path, \"bytes\": speech})\n",
    "\n",
    "        return {\"audio\": audio, \"sentence\": sentence}\n",
    "        # example = processor(audio=audio[\"array\"], sampling_rate=processor.feature_extractor.sampling_rate, text=sentence, return_tensors=\"pt\")\n",
    "        # example[\"input_values\"] = example[\"input_values\"][0]\n",
    "        # example[\"labels\"] = example[\"labels\"][0]\n",
    "        # example[\"input_length\"] = len(audio[\"array\"]) // processor.feature_extractor.sampling_rate\n",
    "        # speech, sr = librosa.load(path, sr=processor.feature_extractor.sampling_rate) \n",
    "#         print(speech.shape)\n",
    "        # return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset= AudioDataset(train_df, data_path=\"bengaliai-speech/train_mp3s\" )\n",
    "test_dataset = AudioDataset(val_df, data_path=\"bengaliai-speech/train_mp3s\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /home/dev/.cache/huggingface/datasets/generator/default-c81e77ba63ca32e0/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a667c26e9fa9475a9b15ae1b6417338f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /home/dev/.cache/huggingface/datasets/generator/default-c81e77ba63ca32e0/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from functools import partial\n",
    "\n",
    "def gen(dset):\n",
    "    for idx in range(len(dset)):\n",
    "        yield dset[idx]  # this has to be a dictionary\n",
    "    ## or if it's an IterableDataset\n",
    "    # for ex in torch_dataset:\n",
    "    #     yield ex\n",
    "\n",
    "train_gen = partial(gen, train_dataset)\n",
    "test_gen = partial(gen, test_dataset)\n",
    "\n",
    "train_dset = Dataset.from_generator(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to /home/dev/.cache/huggingface/datasets/generator/default-e29556b32b5756f1/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a418ac69eb4cb8b9dd2f2ac2c6a639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to /home/dev/.cache/huggingface/datasets/generator/default-e29556b32b5756f1/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "test_dset = Dataset.from_generator(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dset = DatasetDict({\"train\":train_dset,\"validation\": test_dset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n",
      "Resuming upload of the dataset shards.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44293800ef74ecba7317d218974ca43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/2086 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4b3b98813b4a58885fac499cf7acdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/818 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://huggingface.co/api/datasets/Kurokabe/bengaliai-speech/commit/main (Request ID: Root=1-64ccab0d-52308e763136ea3d5cc13f1e;ab145d5d-30f5-4668-b307-7d11ba4e0f7f)\n\nYou have exceeded our hourly quotas for action: commit. We invite you to retry later.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:261\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    262\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/datasets/Kurokabe/bengaliai-speech/commit/main",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m huggingface_dset\u001b[39m.\u001b[39;49mpush_to_hub(\u001b[39m\"\u001b[39;49m\u001b[39mKurokabe/bengaliai-speech\u001b[39;49m\u001b[39m\"\u001b[39;49m, private\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:1635\u001b[0m, in \u001b[0;36mDatasetDict.push_to_hub\u001b[0;34m(self, repo_id, private, token, branch, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   1633\u001b[0m logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPushing split \u001b[39m\u001b[39m{\u001b[39;00msplit\u001b[39m}\u001b[39;00m\u001b[39m to the Hub.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1634\u001b[0m \u001b[39m# The split=key needs to be removed before merging\u001b[39;00m\n\u001b[0;32m-> 1635\u001b[0m repo_id, split, uploaded_size, dataset_nbytes, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m[split]\u001b[39m.\u001b[39;49m_push_parquet_shards_to_hub(\n\u001b[1;32m   1636\u001b[0m     repo_id,\n\u001b[1;32m   1637\u001b[0m     split\u001b[39m=\u001b[39;49msplit,\n\u001b[1;32m   1638\u001b[0m     private\u001b[39m=\u001b[39;49mprivate,\n\u001b[1;32m   1639\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1640\u001b[0m     branch\u001b[39m=\u001b[39;49mbranch,\n\u001b[1;32m   1641\u001b[0m     max_shard_size\u001b[39m=\u001b[39;49mmax_shard_size,\n\u001b[1;32m   1642\u001b[0m     num_shards\u001b[39m=\u001b[39;49mnum_shards\u001b[39m.\u001b[39;49mget(split),\n\u001b[1;32m   1643\u001b[0m     embed_external_files\u001b[39m=\u001b[39;49membed_external_files,\n\u001b[1;32m   1644\u001b[0m )\n\u001b[1;32m   1645\u001b[0m total_uploaded_size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m uploaded_size\n\u001b[1;32m   1646\u001b[0m total_dataset_nbytes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m dataset_nbytes\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:5329\u001b[0m, in \u001b[0;36mDataset._push_parquet_shards_to_hub\u001b[0;34m(self, repo_id, split, private, token, branch, max_shard_size, num_shards, embed_external_files)\u001b[0m\n\u001b[1;32m   5322\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data_files_to_delete):\n\u001b[1;32m   5323\u001b[0m     \u001b[39mfor\u001b[39;00m data_file \u001b[39min\u001b[39;00m logging\u001b[39m.\u001b[39mtqdm(\n\u001b[1;32m   5324\u001b[0m         data_files_to_delete,\n\u001b[1;32m   5325\u001b[0m         desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDeleting unused files from dataset repository\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   5326\u001b[0m         total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(data_files_to_delete),\n\u001b[1;32m   5327\u001b[0m         disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled(),\n\u001b[1;32m   5328\u001b[0m     ):\n\u001b[0;32m-> 5329\u001b[0m         delete_file(data_file)\n\u001b[1;32m   5331\u001b[0m repo_files \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(files) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(data_files_to_delete))\n\u001b[1;32m   5333\u001b[0m \u001b[39mreturn\u001b[39;00m repo_id, split, uploaded_size, dataset_nbytes, repo_files, deleted_size\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:5320\u001b[0m, in \u001b[0;36mDataset._push_parquet_shards_to_hub.<locals>.delete_file\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m   5319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdelete_file\u001b[39m(file):\n\u001b[0;32m-> 5320\u001b[0m     api\u001b[39m.\u001b[39;49mdelete_file(file, repo_id\u001b[39m=\u001b[39;49mrepo_id, token\u001b[39m=\u001b[39;49mtoken, repo_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdataset\u001b[39;49m\u001b[39m\"\u001b[39;49m, revision\u001b[39m=\u001b[39;49mbranch)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:3606\u001b[0m, in \u001b[0;36mHfApi.delete_file\u001b[0;34m(self, path_in_repo, repo_id, token, repo_type, revision, commit_message, commit_description, create_pr, parent_commit)\u001b[0m\n\u001b[1;32m   3600\u001b[0m commit_message \u001b[39m=\u001b[39m (\n\u001b[1;32m   3601\u001b[0m     commit_message \u001b[39mif\u001b[39;00m commit_message \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDelete \u001b[39m\u001b[39m{\u001b[39;00mpath_in_repo\u001b[39m}\u001b[39;00m\u001b[39m with huggingface_hub\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3602\u001b[0m )\n\u001b[1;32m   3604\u001b[0m operations \u001b[39m=\u001b[39m [CommitOperationDelete(path_in_repo\u001b[39m=\u001b[39mpath_in_repo)]\n\u001b[0;32m-> 3606\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_commit(\n\u001b[1;32m   3607\u001b[0m     repo_id\u001b[39m=\u001b[39;49mrepo_id,\n\u001b[1;32m   3608\u001b[0m     repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m   3609\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   3610\u001b[0m     operations\u001b[39m=\u001b[39;49moperations,\n\u001b[1;32m   3611\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   3612\u001b[0m     commit_message\u001b[39m=\u001b[39;49mcommit_message,\n\u001b[1;32m   3613\u001b[0m     commit_description\u001b[39m=\u001b[39;49mcommit_description,\n\u001b[1;32m   3614\u001b[0m     create_pr\u001b[39m=\u001b[39;49mcreate_pr,\n\u001b[1;32m   3615\u001b[0m     parent_commit\u001b[39m=\u001b[39;49mparent_commit,\n\u001b[1;32m   3616\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:828\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_as_future(fn, \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    827\u001b[0m \u001b[39m# Otherwise, call the function normally\u001b[39;00m\n\u001b[0;32m--> 828\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_api.py:2728\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[0;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   2726\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2727\u001b[0m     commit_resp \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mpost(url\u001b[39m=\u001b[39mcommit_url, headers\u001b[39m=\u001b[39mheaders, data\u001b[39m=\u001b[39mdata, params\u001b[39m=\u001b[39mparams)\n\u001b[0;32m-> 2728\u001b[0m     hf_raise_for_status(commit_resp, endpoint_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcommit\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   2729\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   2730\u001b[0m     e\u001b[39m.\u001b[39mappend_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:303\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[39mraise\u001b[39;00m BadRequestError(message, response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[39m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[39m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m \u001b[39mraise\u001b[39;00m HfHubHTTPError(\u001b[39mstr\u001b[39m(e), response\u001b[39m=\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/datasets/Kurokabe/bengaliai-speech/commit/main (Request ID: Root=1-64ccab0d-52308e763136ea3d5cc13f1e;ab145d5d-30f5-4668-b307-7d11ba4e0f7f)\n\nYou have exceeded our hourly quotas for action: commit. We invite you to retry later."
     ]
    }
   ],
   "source": [
    "\n",
    "huggingface_dset.push_to_hub(\"Kurokabe/bengaliai-speech\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Dataset\u001b[39m.\u001b[39;49mfrom_dict(huggingface_dset\u001b[39m.\u001b[39;49mcache_files[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:888\u001b[0m, in \u001b[0;36mDataset.from_dict\u001b[0;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[1;32m    886\u001b[0m features \u001b[39m=\u001b[39m features \u001b[39mif\u001b[39;00m features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m info\u001b[39m.\u001b[39mfeatures \u001b[39mif\u001b[39;00m info \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    887\u001b[0m arrow_typed_mapping \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 888\u001b[0m \u001b[39mfor\u001b[39;00m col, data \u001b[39min\u001b[39;00m mapping\u001b[39m.\u001b[39;49mitems():\n\u001b[1;32m    889\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (pa\u001b[39m.\u001b[39mArray, pa\u001b[39m.\u001b[39mChunkedArray)):\n\u001b[1;32m    890\u001b[0m         data \u001b[39m=\u001b[39m cast_array_to_feature(data, features[col]) \u001b[39mif\u001b[39;00m features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m data\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "Dataset.from_dict(huggingface_dset.cache_files[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "123610be90e44a419192dabfdab2dfad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2086 shards):   0%|          | 0/934048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2c7b2c939945509f98936311ebc088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/91 shards):   0%|          | 0/29588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "huggingface_dset.save_to_disk(\"bengaliai-speech_huggingface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 934048\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'sentence'],\n",
       "        num_rows: 29588\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DatasetDict.load_from_disk(\"bengaliai-speech_huggingface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
