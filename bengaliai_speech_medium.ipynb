{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53dd3d404fbd4fbd9d9dc9dd956f47b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"whisper-medium\"\n",
    "model_name = f\"openai/{model_type}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc3841e432f479596b253890a4b5e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d5212d17b84a3cad3a426fbc45a481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cf4972dad744bd997667e7fdb66b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca2523b0d1a46a899683b1e6cc1a9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b87dc4947194f2fad342c62393c79e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99cc863fa8834a04a01c6db3c8c150a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)main/normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba693545f0ca4343b7ceb4148d7b8a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501842d041c148dab68088e538b2d05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    model_name, language=\"bengali\", task=\"transcribe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "from datasets import Audio\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, labels_df: pd.DataFrame, data_path: str, processor):\n",
    "        self.labels_df = labels_df\n",
    "        self.data_path = data_path\n",
    "        self.processor = processor\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "    def __getitem__(self,idx):\n",
    "        row = self.labels_df.iloc[idx]\n",
    "        path = os.path.join(self.data_path, row[\"id\"]+\".mp3\")\n",
    "        sentence = row[\"sentence\"]\n",
    "        with open(path, \"rb\") as f:\n",
    "            speech = f.read()\n",
    "            audio = Audio(sampling_rate=processor.feature_extractor.sampling_rate).decode_example({\"path\": path, \"bytes\": speech})\n",
    "\n",
    "        example = processor(audio=audio[\"array\"], sampling_rate=processor.feature_extractor.sampling_rate, text=sentence)\n",
    "        example[\"input_length\"] = len(audio[\"array\"]) // processor.feature_extractor.sampling_rate\n",
    "        # speech, sr = librosa.load(path, sr=processor.feature_extractor.sampling_rate) \n",
    "#         print(speech.shape)\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "labels_df = pd.read_csv(\"bengaliai-speech/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = DatasetDict()\n",
    "train_dataset= AudioDataset(labels_df[labels_df[\"split\"] == \"train\"][1751*8:], data_path=\"bengaliai-speech/train_mp3s\", processor=processor)\n",
    "test_dataset = AudioDataset(labels_df[labels_df[\"split\"] == \"valid\"].sample(1000), data_path=\"bengaliai-speech/train_mp3s\", processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice = DatasetDict({\"train\":train_dataset,\"test\": test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'en',\n",
       " 'chinese': 'zh',\n",
       " 'german': 'de',\n",
       " 'spanish': 'es',\n",
       " 'russian': 'ru',\n",
       " 'korean': 'ko',\n",
       " 'french': 'fr',\n",
       " 'japanese': 'ja',\n",
       " 'portuguese': 'pt',\n",
       " 'turkish': 'tr',\n",
       " 'polish': 'pl',\n",
       " 'catalan': 'ca',\n",
       " 'dutch': 'nl',\n",
       " 'arabic': 'ar',\n",
       " 'swedish': 'sv',\n",
       " 'italian': 'it',\n",
       " 'indonesian': 'id',\n",
       " 'hindi': 'hi',\n",
       " 'finnish': 'fi',\n",
       " 'vietnamese': 'vi',\n",
       " 'hebrew': 'he',\n",
       " 'ukrainian': 'uk',\n",
       " 'greek': 'el',\n",
       " 'malay': 'ms',\n",
       " 'czech': 'cs',\n",
       " 'romanian': 'ro',\n",
       " 'danish': 'da',\n",
       " 'hungarian': 'hu',\n",
       " 'tamil': 'ta',\n",
       " 'norwegian': 'no',\n",
       " 'thai': 'th',\n",
       " 'urdu': 'ur',\n",
       " 'croatian': 'hr',\n",
       " 'bulgarian': 'bg',\n",
       " 'lithuanian': 'lt',\n",
       " 'latin': 'la',\n",
       " 'maori': 'mi',\n",
       " 'malayalam': 'ml',\n",
       " 'welsh': 'cy',\n",
       " 'slovak': 'sk',\n",
       " 'telugu': 'te',\n",
       " 'persian': 'fa',\n",
       " 'latvian': 'lv',\n",
       " 'bengali': 'bn',\n",
       " 'serbian': 'sr',\n",
       " 'azerbaijani': 'az',\n",
       " 'slovenian': 'sl',\n",
       " 'kannada': 'kn',\n",
       " 'estonian': 'et',\n",
       " 'macedonian': 'mk',\n",
       " 'breton': 'br',\n",
       " 'basque': 'eu',\n",
       " 'icelandic': 'is',\n",
       " 'armenian': 'hy',\n",
       " 'nepali': 'ne',\n",
       " 'mongolian': 'mn',\n",
       " 'bosnian': 'bs',\n",
       " 'kazakh': 'kk',\n",
       " 'albanian': 'sq',\n",
       " 'swahili': 'sw',\n",
       " 'galician': 'gl',\n",
       " 'marathi': 'mr',\n",
       " 'punjabi': 'pa',\n",
       " 'sinhala': 'si',\n",
       " 'khmer': 'km',\n",
       " 'shona': 'sn',\n",
       " 'yoruba': 'yo',\n",
       " 'somali': 'so',\n",
       " 'afrikaans': 'af',\n",
       " 'occitan': 'oc',\n",
       " 'georgian': 'ka',\n",
       " 'belarusian': 'be',\n",
       " 'tajik': 'tg',\n",
       " 'sindhi': 'sd',\n",
       " 'gujarati': 'gu',\n",
       " 'amharic': 'am',\n",
       " 'yiddish': 'yi',\n",
       " 'lao': 'lo',\n",
       " 'uzbek': 'uz',\n",
       " 'faroese': 'fo',\n",
       " 'haitian creole': 'ht',\n",
       " 'pashto': 'ps',\n",
       " 'turkmen': 'tk',\n",
       " 'nynorsk': 'nn',\n",
       " 'maltese': 'mt',\n",
       " 'sanskrit': 'sa',\n",
       " 'luxembourgish': 'lb',\n",
       " 'myanmar': 'my',\n",
       " 'tibetan': 'bo',\n",
       " 'tagalog': 'tl',\n",
       " 'malagasy': 'mg',\n",
       " 'assamese': 'as',\n",
       " 'tatar': 'tt',\n",
       " 'hawaiian': 'haw',\n",
       " 'lingala': 'ln',\n",
       " 'hausa': 'ha',\n",
       " 'bashkir': 'ba',\n",
       " 'javanese': 'jw',\n",
       " 'sundanese': 'su',\n",
       " 'burmese': 'my',\n",
       " 'valencian': 'ca',\n",
       " 'flemish': 'nl',\n",
       " 'haitian': 'ht',\n",
       " 'letzeburgesch': 'lb',\n",
       " 'pushto': 'ps',\n",
       " 'panjabi': 'pa',\n",
       " 'moldavian': 'ro',\n",
       " 'moldovan': 'ro',\n",
       " 'sinhalese': 'si',\n",
       " 'castilian': 'es'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.whisper.tokenization_whisper import TO_LANGUAGE_CODE\n",
    "\n",
    "TO_LANGUAGE_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(\n",
    "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [\n",
    "            {\"input_features\": feature[\"input_features\"][0]} for feature in features\n",
    "        ]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    # compute orthographic wer\n",
    "    wer_ortho = metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    # compute normalised WER\n",
    "    pred_str_norm = [normalizer(pred) for pred in pred_str]\n",
    "    label_str_norm = [normalizer(label) for label in label_str]\n",
    "    # filtering step to only evaluate the samples that correspond to non-zero references:\n",
    "    pred_str_norm = [\n",
    "        pred_str_norm[i] for i in range(len(pred_str_norm)) if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "    label_str_norm = [\n",
    "        label_str_norm[i]\n",
    "        for i in range(len(label_str_norm))\n",
    "        if len(label_str_norm[i]) > 0\n",
    "    ]\n",
    "\n",
    "    wer = metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
    "\n",
    "    return {\"wer_ortho\": wer_ortho, \"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9476830ab6de43ce8f58d791721adfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.99k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6feb17501dba43d487bb8722dd88fc8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/3.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589e47d8060a439b8cf9541b153c94f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/3.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# disable cache during training since it's incompatible with gradient checkpointing\n",
    "model.config.use_cache = False\n",
    "\n",
    "# set language and task for generation and re-enable cache\n",
    "model.generate = partial(\n",
    "    model.generate, language=\"bengali\", task=\"transcribe\", use_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import shutil\n",
    "import json\n",
    "import kaggle\n",
    "from pathlib import Path\n",
    "from transformers import TrainerCallback\n",
    "from transformers.trainer_callback import TrainerControl, TrainerState\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "class KaggleUploader(TrainerCallback):\n",
    "    def __init__(self, dataset_path: str, id: str, title: str, isPrivate: bool):\n",
    "        self.api = kaggle.KaggleApi()\n",
    "        self.api.authenticate()\n",
    "        self.dataset_path = dataset_path\n",
    "        self.meta_data = dict(\n",
    "            id=id,\n",
    "            title=title,\n",
    "            isPrivate=isPrivate,\n",
    "            licenses=[dict(name=\"other\")]\n",
    "        )\n",
    "\n",
    "    def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        best_model_checkpoint = str(Path(state.best_model_checkpoint).name)\n",
    "        self.upload_dataset_to_kaggle(self.dataset_path, best_model_checkpoint)\n",
    "        \n",
    "        return super().on_evaluate(args, state, control, **kwargs)\n",
    "    \n",
    "    def upload_dataset_to_kaggle(self, dataset_path, checkpoint_to_save: str):\n",
    "        # latest_checkpoint = find_latest_checkpoint(dataset_path)\n",
    "        checkpoint = os.path.join(dataset_path, checkpoint_to_save)\n",
    "\n",
    "        version_notes = checkpoint_to_save\n",
    "        # The checkpoint has multiple files that we don't need.\n",
    "        # We only need the pytorch_model.bin file, config.json and generation_config.json\n",
    "        # Copy these files to a temporary folder\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # create a directory inside named \"model\"\n",
    "            temp_model_dir = os.path.join(temp_dir, \"model\")\n",
    "            os.mkdir(temp_model_dir)\n",
    "            # copy the files\n",
    "            for file in [\"pytorch_model.bin\", \"config.json\", \"generation_config.json\"]:\n",
    "                shutil.copy(os.path.join(checkpoint, file), temp_model_dir)\n",
    "\n",
    "            # create dataset-metadata.json inside the temporary directory\n",
    "            with open(os.path.join(temp_model_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "                json.dump(self.meta_data, f)\n",
    "\n",
    "            self.api.dataset_create_version(temp_model_dir, version_notes=version_notes, dir_mode=\"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_uploader = KaggleUploader(\"/workspaces/HuggingFace-Audio-Course/whisper-medium-bn\",\n",
    "                                 id=\"kurokabe/whisper-medium-bn\",\n",
    "                                 title=\"Whisper medium bengali\", \n",
    "                                 isPrivate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"./{model_type}-bn\",  # name on the HF Hub\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=8,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=5e-6,\n",
    "    lr_scheduler_type=\"constant_with_warmup\",\n",
    "    warmup_steps=50,\n",
    "    max_steps=10000,  # increase to 4000 if you have your own GPU or a Colab paid plan\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    fp16_full_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    "    dataloader_num_workers=64,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/HuggingFace-Audio-Course/./whisper-medium-bn is already a clone of https://huggingface.co/Kurokabe/whisper-medium-bn. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    "    callbacks=[kaggle_uploader]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_config.max_length = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        trainer.train(resume_from_checkpoint=True)\n",
    "    except RuntimeError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b31e52e39184daf87873b9ba04d68c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/2.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/Kurokabe/whisper-medium-bn\n",
      "   5456f02..52a5360  main -> main\n",
      "\n",
      "To https://huggingface.co/Kurokabe/whisper-medium-bn\n",
      "   52a5360..615b63d  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/Kurokabe/whisper-medium-bn/commit/52a536006c9715eeade5a9a84d5b4f069523de0c'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"model_name\": f\"{model_type} bn\",  # a 'pretty' name for your model\n",
    "    \"language\": \"bn\",\n",
    "    \"finetuned_from\": model_name,\n",
    "    \"tasks\": \"automatic-speech-recognition\",\n",
    "}\n",
    "trainer.push_to_hub(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_latest_checkpoint(dataset_path):\n",
    "#     # checkpoints are folders of the following format: checkpoint-XXXXX\n",
    "#     checkpoints = [\n",
    "#         folder\n",
    "#         for folder in os.listdir(dataset_path)\n",
    "#         if folder.startswith(\"checkpoint-\")\n",
    "#     ]\n",
    "#     # sort the checkpoints by their step number\n",
    "#     checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "#     # return the path to the latest checkpoint\n",
    "#     return os.path.join(dataset_path, checkpoints[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
