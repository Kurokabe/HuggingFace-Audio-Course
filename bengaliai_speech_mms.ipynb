{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"mms-1b-all\"\n",
    "model_name = f\"facebook/{model_type}\"\n",
    "repo_name = f\"{model_type}-bengali\"\n",
    "num_val_samples = 1024 * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3b32fde2814f0e9158086b439fd1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lang = \"ben\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2CTCTokenizer\n",
    "\n",
    "mms_adapter_repo = \"facebook/mms-1b-all\"  # make sure to replace this path with a repo to which you want to add your new adapter weights\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "processor.tokenizer.set_target_lang(target_lang)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "## Kaggle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Audio\n",
    "\n",
    "bengali_dataset = DatasetDict.load_from_disk(\"/data/abdalla/bengaliai-speech_huggingface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # batched output is \"un-batched\"\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=16_000).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "    batch[\"labels\"] = processor(text=batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bengali_dataset[\"train\"] = bengali_dataset[\"train\"].map(prepare_dataset, remove_columns=bengali_dataset[\"train\"].column_names, num_proc=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bengali_dataset[\"validation\"] = bengali_dataset[\"validation\"].map(prepare_dataset, remove_columns=bengali_dataset[\"validation\"].column_names, num_proc=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mozilla Common Voice Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:2069: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=True' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8352e650c4754b3ba2bb71693dd6a2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222a7e0522bf433ebc030f43419ac24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/14.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854a16dd5ca54fc69630695ddc306847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.65k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd6b55667094a26b20e2e6502a816e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/65.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f095e86002a04ffdbcbbf7f1baab494d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea495419abd46b38c491fab0ca3870c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407ea2c174724519a71a921a5226479a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/733M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a4581157a54fffb05ad7823cfe885b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/355M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4b764bb5f44492a62dfe444f502ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/363M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ff921467274ba28324e12b1bdd6714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b86c535301140e7b7655d33f50f458f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b418eb371c49ef876a3ccda147bcbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.53G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric, Audio\n",
    "\n",
    "common_voice_train = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"bn\", split=\"train\", download_mode=\"force_redownload\", use_auth_token=True)\n",
    "common_voice_val = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"bn\", split=\"test\", download_mode=\"force_redownload\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\", \"path\", \"variant\"])\n",
    "common_voice_val = common_voice_val.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\", \"path\", \"variant\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_train = common_voice_train.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "common_voice_val = common_voice_val.cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/dev/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/bn/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-691f3a9e33d108b9.arrow\n",
      "Loading cached processed dataset at /home/dev/.cache/huggingface/datasets/mozilla-foundation___common_voice_13_0/bn/13.0.0/2506e9a8950f5807ceae08c2920e814222909fd7f477b74f5d225802e9f04055/cache-1fbfd44b063357b5.arrow\n"
     ]
    }
   ],
   "source": [
    "common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names)\n",
    "common_voice_val = common_voice_val.map(prepare_dataset, remove_columns=common_voice_val.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "combined_train = concatenate_datasets([bengali_dataset[\"train\"], common_voice_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_val = concatenate_datasets([bengali_dataset[\"validation\"], common_voice_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\"train\": combined_train, \"validation\": combined_val})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<Axes: title={'center': '0'}>]], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(dataset[\"validation\"][\"input_length\"]).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pd\u001b[39m.\u001b[39mDataFrame(dataset[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39minput_length\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(dataset[\"validation\"][\"input_length\"]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_length(batch):\n",
    "    return batch[\"input_length\"] <= 180864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /workspaces/HuggingFace-Audio-Course/bengaliai-speech_huggingface/train/cache-7beae8d9fa1655fb_*_of_00064.arrow\n",
      "Loading cached processed dataset at /workspaces/HuggingFace-Audio-Course/bengaliai-speech_huggingface/validation/cache-db7c401f0d345f3e_*_of_00064.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(filter_by_length, num_proc=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"validation\"] = dataset[\"validation\"].select(pd.Series(list(range(len(dataset[\"validation\"])))).sample(num_val_samples).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# def filter_invalid(batch):\n",
    "#     return torch.tensor(batch[\"input_values\"]).isfinite().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_voice_train.filter(filter_invalid, num_proc=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# chars_to_remove_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�\\'\\‚\\/]'\n",
    "\n",
    "# def remove_special_characters(batch):\n",
    "#     batch[\"sentence\"] = re.sub(chars_to_remove_regex, '', batch[\"sentence\"]).lower()\n",
    "#     return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_voice_train = common_voice_train.map(remove_special_characters)\n",
    "# common_voice_test = common_voice_test.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_all_chars(batch):\n",
    "#   all_text = \" \".join(batch[\"sentence\"])\n",
    "#   vocab = list(set(all_text))\n",
    "#   return {\"vocab\": [vocab], \"all_text\": [all_text]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_train = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_train.column_names)\n",
    "# vocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(batch):\n",
    "#     audio = batch[\"audio\"]\n",
    "\n",
    "#     # batched output is \"un-batched\"\n",
    "#     batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "#     batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "#     batch[\"labels\"] = processor(text=batch[\"sentence\"]).input_ids\n",
    "#     return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_voice_train = common_voice_train.map(prepare_dataset, remove_columns=common_voice_train.column_names)\n",
    "# common_voice_test = common_voice_test.map(prepare_dataset, remove_columns=common_voice_test.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from torch.utils.data import Dataset\n",
    "# from datasets import Audio\n",
    "\n",
    "# class AudioDataset(Dataset):\n",
    "#     def __init__(self, labels_df: pd.DataFrame, data_path: str, processor):\n",
    "#         self.labels_df = labels_df\n",
    "#         self.data_path = data_path\n",
    "#         self.processor = processor\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels_df)\n",
    "#     def __getitem__(self,idx):\n",
    "#         row = self.labels_df.iloc[idx]\n",
    "#         path = os.path.join(self.data_path, row[\"id\"]+\".mp3\")\n",
    "#         sentence = row[\"sentence\"]\n",
    "#         with open(path, \"rb\") as f:\n",
    "#             speech = f.read()\n",
    "#             audio = Audio(sampling_rate=processor.feature_extractor.sampling_rate).decode_example({\"path\": path, \"bytes\": speech})\n",
    "\n",
    "#         example = processor(audio=audio[\"array\"], sampling_rate=processor.feature_extractor.sampling_rate, text=sentence, return_tensors=\"pt\")\n",
    "#         example[\"input_values\"] = example[\"input_values\"][0]\n",
    "#         example[\"labels\"] = example[\"labels\"][0]\n",
    "#         example[\"input_length\"] = len(audio[\"array\"]) // processor.feature_extractor.sampling_rate\n",
    "#         # speech, sr = librosa.load(path, sr=processor.feature_extractor.sampling_rate) \n",
    "# #         print(speech.shape)\n",
    "#         return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import DatasetDict\n",
    "# labels_df = pd.read_csv(\"bengaliai-speech/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train_df, val_df = train_test_split(labels_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_voice = DatasetDict()\n",
    "# train_dataset= AudioDataset(train_df, data_path=\"bengaliai-speech/train_mp3s\", processor=processor)\n",
    "# test_dataset = AudioDataset(val_df, data_path=\"bengaliai-speech/train_mp3s\", processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_voice = DatasetDict({\"train\":train_dataset,\"test\": test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.pad(\n",
    "            labels=label_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "wer_metric = load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/mms-1b-all and are newly initialized because the shapes did not match:\n",
      "- lm_head.bias: found shape torch.Size([154]) in the checkpoint and torch.Size([136]) in the model instantiated\n",
      "- lm_head.weight: found shape torch.Size([154, 1280]) in the checkpoint and torch.Size([136, 1280]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    model_name,\n",
    "    attention_dropout=0.0,\n",
    "    hidden_dropout=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    layerdrop=0.0,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_adapter_layers()\n",
    "model.freeze_base_model()\n",
    "\n",
    "adapter_weights = model._get_adapters()\n",
    "for param in adapter_weights.values():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import json\n",
    "import kaggle\n",
    "from pathlib import Path\n",
    "from transformers import TrainerCallback\n",
    "from transformers.trainer_callback import TrainerControl, TrainerState\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "class KaggleUploader(TrainerCallback):\n",
    "    def __init__(self, dataset_path: str, id: str, title: str, isPrivate: bool):\n",
    "        self.api = kaggle.KaggleApi()\n",
    "        self.api.authenticate()\n",
    "        self.dataset_path = dataset_path\n",
    "        self.meta_data = dict(\n",
    "            id=id,\n",
    "            title=title,\n",
    "            isPrivate=isPrivate,\n",
    "            licenses=[dict(name=\"other\")]\n",
    "        )\n",
    "        self.previous_best = None\n",
    "\n",
    "    def on_save(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        best_model_checkpoint = str(Path(state.best_model_checkpoint).name)\n",
    "        print(state.best_model_checkpoint)\n",
    "        if best_model_checkpoint != self.previous_best:\n",
    "            self.upload_dataset_to_kaggle(self.dataset_path, best_model_checkpoint)\n",
    "            self.previous_best = best_model_checkpoint\n",
    "        \n",
    "        return super().on_evaluate(args, state, control, **kwargs)\n",
    "    \n",
    "    def upload_dataset_to_kaggle(self, dataset_path, checkpoint_to_save: str):\n",
    "        # latest_checkpoint = find_latest_checkpoint(dataset_path)\n",
    "        checkpoint = os.path.join(dataset_path, checkpoint_to_save)\n",
    "\n",
    "        version_notes = checkpoint_to_save\n",
    "        # The checkpoint has multiple files that we don't need.\n",
    "        # We only need the pytorch_model.bin file, config.json and generation_config.json\n",
    "        # Copy these files to a temporary folder\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # create a directory inside named \"model\"\n",
    "            temp_model_dir = os.path.join(temp_dir, \"model\")\n",
    "            os.mkdir(temp_model_dir)\n",
    "            # copy the files\n",
    "            for file in [\"pytorch_model.bin\", \"config.json\"]:\n",
    "                shutil.copy(os.path.join(checkpoint, file), temp_model_dir)\n",
    "\n",
    "            # create dataset-metadata.json inside the temporary directory\n",
    "            with open(os.path.join(temp_model_dir, \"dataset-metadata.json\"), \"w\") as f:\n",
    "                json.dump(self.meta_data, f)\n",
    "\n",
    "            self.api.dataset_create_version(temp_model_dir, version_notes=version_notes, dir_mode=\"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_uploader = KaggleUploader(\"/workspaces/HuggingFace-Audio-Course/mms-1b-all-bengali\",\n",
    "                                 id=\"kurokabe/mms-1b-model\",\n",
    "                                 title=\"MMS 1B bengali\", \n",
    "                                 isPrivate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=repo_name,\n",
    "  group_by_length=True,\n",
    "  length_column_name=\"input_length\",\n",
    "  per_device_train_batch_size=32,\n",
    "  per_device_eval_batch_size=32,\n",
    "  # gradient_accumulation_steps=4,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=2,\n",
    "  # max_steps=500,\n",
    "  gradient_checkpointing=True,\n",
    "  fp16=True,\n",
    "  save_steps=250,\n",
    "  eval_steps=250,\n",
    "  logging_steps=100,\n",
    "  learning_rate=5e-6,\n",
    "  warmup_steps=100,\n",
    "  save_total_limit=2,\n",
    "  # push_to_hub=True,\n",
    "  metric_for_best_model=\"wer\",\n",
    "  greater_is_better=False,\n",
    "  load_best_model_at_end=True,\n",
    "  # debug=\"underflow_overflow\"\n",
    "  # dataloader_num_workers=64,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    callbacks=[kaggle_uploader]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4751' max='44764' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4751/44764 6:52:11 < 99:59:43, 0.11 it/s, Epoch 0.21/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>2.331800</td>\n",
       "      <td>1.384291</td>\n",
       "      <td>0.781540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.187000</td>\n",
       "      <td>1.229206</td>\n",
       "      <td>0.741407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2.038600</td>\n",
       "      <td>1.051850</td>\n",
       "      <td>0.689443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.884800</td>\n",
       "      <td>0.912545</td>\n",
       "      <td>0.645039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.877500</td>\n",
       "      <td>0.916694</td>\n",
       "      <td>0.632391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.901400</td>\n",
       "      <td>0.880957</td>\n",
       "      <td>0.621890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.882500</td>\n",
       "      <td>0.917044</td>\n",
       "      <td>0.625393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.933100</td>\n",
       "      <td>0.918206</td>\n",
       "      <td>0.624537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>1.951500</td>\n",
       "      <td>0.918206</td>\n",
       "      <td>0.624537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.934800</td>\n",
       "      <td>0.918206</td>\n",
       "      <td>0.624537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='420' max='512' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [420/512 19:03 < 04:11, 0.37 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mms-1b-all-bengali/checkpoint-2250\n",
      "Starting upload for file pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3.59G/3.59G [01:38<00:00, 39.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: pytorch_model.bin (4GB)\n",
      "Starting upload for file config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.00k/2.00k [00:00<00:00, 3.20kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: config.json (2KB)\n",
      "mms-1b-all-bengali/checkpoint-2500\n",
      "Starting upload for file pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3.59G/3.59G [01:25<00:00, 45.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: pytorch_model.bin (4GB)\n",
      "Starting upload for file config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.00k/2.00k [00:00<00:00, 3.29kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: config.json (2KB)\n",
      "mms-1b-all-bengali/checkpoint-2750\n",
      "Starting upload for file pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3.59G/3.59G [01:38<00:00, 39.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: pytorch_model.bin (4GB)\n",
      "Starting upload for file config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.00k/2.00k [00:00<00:00, 3.24kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: config.json (2KB)\n",
      "mms-1b-all-bengali/checkpoint-3000\n",
      "Starting upload for file pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3.59G/3.59G [01:35<00:00, 40.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: pytorch_model.bin (4GB)\n",
      "Starting upload for file config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.00k/2.00k [00:00<00:00, 3.50kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: config.json (2KB)\n",
      "mms-1b-all-bengali/checkpoint-3250\n",
      "Starting upload for file pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3.59G/3.59G [01:36<00:00, 39.8MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: pytorch_model.bin (4GB)\n",
      "Starting upload for file config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.00k/2.00k [00:00<00:00, 2.11kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: config.json (2KB)\n",
      "mms-1b-all-bengali/checkpoint-3500\n",
      "Starting upload for file pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3.59G/3.59G [01:23<00:00, 46.4MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: pytorch_model.bin (4GB)\n",
      "Starting upload for file config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2.00k/2.00k [00:01<00:00, 1.86kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: config.json (2KB)\n",
      "mms-1b-all-bengali/checkpoint-3500\n",
      "mms-1b-all-bengali/checkpoint-3500\n",
      "mms-1b-all-bengali/checkpoint-3500\n",
      "mms-1b-all-bengali/checkpoint-3500\n"
     ]
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=\"/workspaces/HuggingFace-Audio-Course/mms-1b-all-bengali/checkpoint-3500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: তাই যুদ্ধের শুরুতেকেই পাকিস্তানী বাহিনী বাছাই করে করে বুদ্ধিজীবিদের হত্তা করতে থাকে।\n"
     ]
    }
   ],
   "source": [
    "test_input = torch.tensor(combined_val[0][\"input_values\"]).unsqueeze(0)\n",
    "# cast to torch.float16\n",
    "test_input = test_input.to(\"cuda\")\n",
    "logits = model(test_input).logits\n",
    "pred_ids = torch.argmax(logits, dim=-1)[0]\n",
    "print(\"Prediction:\", processor.tokenizer.decode(pred_ids, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from safetensors.torch import save_file as safe_save_file\n",
    "# from transformers.models.wav2vec2.modeling_wav2vec2 import WAV2VEC2_ADAPTER_SAFE_FILE\n",
    "# import os\n",
    "\n",
    "# adapter_file = WAV2VEC2_ADAPTER_SAFE_FILE.format(target_lang)\n",
    "# adapter_file = os.path.join(training_args.output_dir, adapter_file)\n",
    "\n",
    "# safe_save_file(model._get_adapters(), adapter_file, metadata={\"format\": \"pt\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c538d7c74aa427ba0978347c21b3c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 1.00/3.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e025da171ba4472a4b65f1e6d0449f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Aug01_15-37-33_772b1c8fe2c9/events.out.tfevents.1690904260.772b1c8fe2c9:   0%|          | 1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2aefadd150c4b549ee7d37ca8288b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Aug01_15-26-34_772b1c8fe2c9/events.out.tfevents.1690903598.772b1c8fe2c9:   0%|          | 1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04aa581fb6a9441aac0f463a2a1dda97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Aug01_15-28-33_772b1c8fe2c9/events.out.tfevents.1690903717.772b1c8fe2c9:   0%|          | 1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e5cbd8837d402486beb8f8a9fa59d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Aug01_15-27-39_772b1c8fe2c9/events.out.tfevents.1690903662.772b1c8fe2c9:   0%|          | 1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f130b6e94bd940b499e300bb0d963592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Aug01_15-43-36_772b1c8fe2c9/events.out.tfevents.1690904686.772b1c8fe2c9:   0%|          | 1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3cde5b61dc4dac82e2261b9dce6cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Aug01_15-25-32_772b1c8fe2c9/events.out.tfevents.1690903536.772b1c8fe2c9:   0%|          | 1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d54e81eb284ababd65fdbfe7739311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file runs/Aug01_15-30-58_772b1c8fe2c9/events.out.tfevents.1690903867.772b1c8fe2c9:   0%|          | 1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f1973e4dad4ef298d5be8526391f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file training_args.bin:   0%|          | 1.00/3.87k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/Kurokabe/mms-1b-all-bengali\n",
      "   2b7c63c..a06b654  main -> main\n",
      "\n",
      "To https://huggingface.co/Kurokabe/mms-1b-all-bengali\n",
      "   a06b654..ac833ad  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/Kurokabe/mms-1b-all-bengali/commit/a06b654b114e0ebff3d896de3f3a1b0067fded11'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
